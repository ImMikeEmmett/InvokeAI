{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy86JeXPHQmW"
      },
      "source": [
        "# InvokeAI in Google Colab\n",
        "\n",
        "This is a tool to use Google colab to run the AI image generation tool: Invokeai (https://invoke-ai.github.io/InvokeAI/).\n",
        "This automatically builds itself and can import custom models, It can connect to Google drive to save your images.\n",
        "It also has the option of running entirely from Google drive to vastly speed up the time it takes to start, However, over 20Gb of Google Drive space is required! However it changes subsequent startup times from 15-20 mins to 5-6 mins.\n",
        "\n",
        "Make sure to enable GPU, Edit > Notebook Settings > Hardware accelerator > GPU\n",
        "\n",
        "To start, Click \"Runtime\" > \"Run All\". Alternaivly you can click the \"play\" button on each step below one after the other, No need to wait for the previous steps to finish as they will join a queue.<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cEAEBY2sFdGR"
      },
      "outputs": [],
      "source": [
        "#@title 1. Configuration { display-mode: \"form\" }\n",
        "#@markdown #Instance Type\n",
        "#@markdown **Google_Drive** = Stores ALL files in your Google drive, This can take over 20Gb of space (dependent on your settings), but will start up much faster as it does not download invokeai at each run. To make it download a fresh version simply delete the folder \"/content/drive/MyDrive/invokeai/invokeaiV3/noUpdate, or indeed the whole \"invokeaiV3\" folder as outputs are stored elsewhere.<br>\n",
        "#@markdown **Persistent** = Stores only your images in Google drive, in a folder called \"/invokeai/invokeaiOutputV3\".<br>\n",
        "#@markdown **Temporary** (NOT recomended) = Everything is stored in the runtime and is removed when the runtime ends or crashes, make sure to download your images! <br><br>\n",
        "#@markdown **Rough Startup times:** <br>\n",
        "#@markdown Google_Drive, First run, or any \"Persistent/Temporary\" run = 15-20 mins <br>\n",
        "#@markdown Google_Drive, Subsiquent runs, 6-7 mins.\n",
        "Type = \"Google_Drive\" #@param ['Google_Drive','Persistent','Temporary'] {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #Connection Type.\n",
        "#@markdown Localtunnel: Stable once connected, but sometimes has issues.<br>\n",
        "#@markdown Serveo: less stable, requires no setup. As an alternative to Localtunnel<br>\n",
        "#@markdown NGROK: Highly stable but needs a little setting up, An NGROK token is required, sign up for free and get one here: https://dashboard.ngrok.com/get-started/your-authtoken Once you have the token, please put it in below.<br>\n",
        "connection_type = \"Localtunnel\"  #@param [\"Serveo\",\"Localtunnel\",\"NGROK\"]\n",
        "ngrok_token = \"None\" #@param ['None'] {allow-input: true}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #NSFW Checker\n",
        "#@markdown This checks outputs for \"Not Safe For Work\" content and obscures the image. This is not 100% reliable.\n",
        "nsfw_checker = \"Enabled\" #@param ['Enabled' ,'Disabled'] {type:\"string\"}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Models { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Skip Default Models.<br><br>\n",
        "#@markdown If you are not intending to use them, it is HIGHLY recomended to skip the default models. as this will save 20+GB of downloads and about 10 mins of loading.<br>\n",
        "skip_default_models = \"No\" #@param [\"Yes\",\"No\"]\n",
        "\n",
        "#@markdown With all custom model types if you are using a \"Google_Drive\" type install, you can run this once to generate the folder structure and manually upload models into google drive into the folder /invokeai/invokeaiV3/autoimport/RELAVENT_FOLDER (e.g. /autoimport/vae) or you can use one of the below methods to add models into any install type.<br>\n",
        "\n",
        "#@markdown Custom Diffuser Model\n",
        "custom_diffuser_hugging_face_id = \"None\" #@param [\"None\",\"kebab111/crosskemono\", \"dreamlike-art/dreamlike-photoreal-2.0\", \"stablediffusionapi/disney-pixal-cartoon\"] {allow-input: true}\n",
        "#@markdown Custom Safetensor or Checkpoint models <br>\n",
        "custom_model_url = \"None\" #@param [\"None\",\"https://huggingface.co/ffggggfg/Crosskemono/resolve/main/crosskemonoFurryModel_crosskemono25.safetensors\"] {allow-input: true}\n",
        "custom_vae_url = \"None\" #@param [\"None\",\"https://huggingface.co/ffggggfg/Crosskemono/resolve/main/Crosskemono2.vae.pt\"] {allow-input: true}\n",
        "\n",
        "# #@markdown If you have selected a custom diffuser model it will be auto imported and it is selectable from the dropdown menu in app. <br>\n",
        "# #@markdown Custom .safetensor or .checkpoint models are stored in /content/invokeai/CustomModels and must be manually added via the GUI after it starts. Your Custom VAE will be stored in /content/invokeai/CustomVae <br>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown LoRA <br>\n",
        "#@markdown To import a LoRA please please input a hugging face repo ID or the URL to your LoRA file.<br>\n",
        "HuggingFace_LoRA_Repo_ID = \"None\" #@param [\"None\", \"artificialguybr/LogoRedmond-LogoLoraForSDXL\"] {allow-input: true}\n",
        "custom_LoRA_url = \"None\" #@param [\"None\"] {allow-input: true}\n",
        "\n",
        "\n",
        "#@markdown ControlNet <br>\n",
        "#@markdown To import a ControlNet please input a hugging face repo ID or the URL to your ControlNet file.<br>\n",
        "HuggingFace_ControlNet_Repo_ID = \"None\" #@param [\"None\", \"CrucibleAI/ControlNetMediaPipeFace\"] {allow-input: true}\n",
        "custom_ControlNet_url = \"None\" #@param [\"None\"] {allow-input: true}\n"
      ],
      "metadata": {
        "id": "g9611wcnE3e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. Advanced Options { display-mode: \"form\" }\n",
        "\n",
        "#@markdown #Xformers\n",
        "#@markdown Xformers makes this \"memory efficient\" so larger images (above 1024x1024) can be made, However it can be less stable and may generate blank outputs. It is recomended that you set it to \"Enabled\" unless you experiance issues with your models.\n",
        "xformers = \"Enabled\" #@param ['Enabled' ,'Disabled'] {type:\"string\"}\n",
        "\n",
        "#@markdown #Version\n",
        "#@markdown By default this application uses \"The latest official release of InvokeAI\" however if you wish to use a specific version please enter its version code below.<br>\n",
        "#@markdown Please be aware, If you are using a \"Google_Drive\" version of this program. DOWNGRADING is not officially supported. It may work, it may not, If this does break InvokeAI please DELETE the whole folder /Google_Drive/InvokeAI/invokeaiV3 from https://drive.google.com/, If required please backup your \"Autoimport\" folder. <br>\n",
        "#@markdown Some recomeneded older versions can be located in the dropdown. but any version 3.0.2a1 or newer should work.\n",
        "Version = \"Default\" #@param [\"Default\",\"3.0.2a1\",\"3.0.2.post1\"] {allow-input: true}\n",
        "\n"
      ],
      "metadata": {
        "id": "6WDsQHKoEj7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4. Build and Configure, Invokeai V3 { display-mode: \"form\" }\n",
        "#@markdown This takes about 15 mins to finish + some extra time to download custom models. <br>\n",
        "#@markdown Version 3 Downloads all models during the build phase.<br>\n",
        "#@markdown It will auto add all models to the app.\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#Set up temporary storage if running in \"Temporary\" mode.\n",
        "if Type == \"Temporary\":\n",
        "    file_path = '/content/invokeai'\n",
        "    outpath = '/content/invokeai/output'\n",
        "    noUpdate = '/content/invokeai/noUpdate'\n",
        "    import os\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "\n",
        "# Mount and set up Google drive if running in \"Persistent\" mode.\n",
        "if Type == \"Persistent\":\n",
        "  file_path = '/content/invokeai'\n",
        "  outpath = '/content/drive/MyDrive/invokeai/invokeaiOutputV3'\n",
        "  noUpdate = '/content/invokeai/noUpdate'\n",
        "  import os\n",
        "  from google.colab import drive\n",
        "  if not os.path.exists(file_path):\n",
        "    drive.mount('/content/drive')\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "\n",
        "# Mount and set up Google drive if running in \"Google_Drive\" mode.\n",
        "if Type == \"Google_Drive\":\n",
        "  file_path = '/content/drive/MyDrive/invokeai/invokeaiV3'\n",
        "  outpath = '/content/drive/MyDrive/invokeai/invokeaiOutputV3'\n",
        "  noUpdate = '/content/drive/MyDrive/invokeai/invokeaiV3/noUpdate'\n",
        "\n",
        "  import os\n",
        "  from google.colab import drive\n",
        "  if not os.path.exists(file_path):\n",
        "    drive.mount('/content/drive')\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "\n",
        "#Install Python + Pip\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.11 python3-pip python3.11-distutils python3.11-dev libpython3.11-dev python3.11-venv -y\n",
        "\n",
        "#Update pip\n",
        "%cd {file_path}\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!python3 get-pip.py --force-reinstall\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install --upgrade setuptools\n",
        "\n",
        "#Install some Google colab dependencies.\n",
        "!python3 -m pip install ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "#Create InvokeAI root\n",
        "import os\n",
        "os.environ['INVOKEAI_ROOT'] = file_path\n",
        "if not os.path.exists(file_path):\n",
        "  os.makedirs(invokeai_root)\n",
        "\n",
        "#Create the virtual environment + Download default Models\n",
        "%cd {file_path}\n",
        "\n",
        "#On 2nd run, Do an \"upgrade\" to get system variables to load quickly.\n",
        "if os.path.exists(noUpdate):\n",
        "  !python -m venv .venv --prompt InvokeAI\n",
        "  if Version==\"Default\":\n",
        "    !source .venv/bin/activate; python -m pip install InvokeAI[xformers] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
        "  if Version != \"Default\":\n",
        "    !source .venv/bin/activate; python -m pip install InvokeAI[xformers]=={Version} --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade\n",
        "\n",
        "\n",
        "#PIP First time install of InvokeAI.\n",
        "if not os.path.exists(noUpdate):\n",
        "  !python -m venv .venv --prompt InvokeAI\n",
        "  !source .venv/bin/activate; python -m pip install --upgrade pip\n",
        "\n",
        "  if Version==\"Default\":\n",
        "    !source .venv/bin/activate; python -m pip install InvokeAI[xformers] --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "  if Version != \"Default\":\n",
        "    !source .venv/bin/activate; python -m pip install InvokeAI[xformers]=={Version} --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "  !mkdir {outpath}\n",
        "  !cd {file_path}\n",
        "\n",
        "  if skip_default_models == \"Yes\":\n",
        "    !source .venv/bin/activate; invokeai-configure --yes --skip-sd-weights\n",
        "  if skip_default_models == \"No\":\n",
        "    !source .venv/bin/activate; invokeai-configure --yes\n",
        "\n",
        "  !mkdir {noUpdate}\n",
        "\n",
        "## Edit invokeai.yaml\n",
        "!sed -i 's/max_loaded_models: 3/max_loaded_models: 2/' invokeai.yaml\n",
        "!sed -i 's/max_cache_size: 6.0/max_cache_size: 6.0/' invokeai.yaml\n",
        "!sed -i 's/max_vram_cache_size: 2.75/max_vram_cache_size: 4.0/' invokeai.yaml\n",
        "\n",
        "#Toggle Xformers Mode\n",
        "if xformers == \"Disabled\":\n",
        "  !sed -i 's/xformers_enabled: true/xformers_enabled: false/' invokeai.yaml\n",
        "if xformers == \"Enabled\":\n",
        "  !sed -i 's/xformers_enabled: false/xformers_enabled: true/' invokeai.yaml\n",
        "\n",
        "#Toggle the NSFW checker\n",
        "if nsfw_checker == \"Disabled\":\n",
        "  !sed -i 's/nsfw_checker: true/nsfw_checker: false/' invokeai.yaml\n",
        "if nsfw_checker == \"Enabled\":\n",
        "  !sed -i 's/nsfw_checker: false/nsfw_checker: true/' invokeai.yaml\n",
        "\n",
        "#Download custom checkpoint models\n",
        "if custom_model_url != \"None\":\n",
        "  %cd {file_path}/autoimport/main\n",
        "  !wget -nc {file_path}/autoimport/main {custom_model_url}\n",
        "\n",
        "#Download custom VAE\n",
        "if custom_vae_url != \"None\":\n",
        "  %cd {file_path}/autoimport/vae\n",
        "  !wget -nc {file_path}/autoimport/vae {custom_vae_url}\n",
        "\n",
        "#Import Diffuser Model\n",
        "if custom_diffuser_hugging_face_id != \"None\":\n",
        "  %cd {file_path}/autoimport/main\n",
        "  !git clone --depth 1 --verbose --progress https://huggingface.co/{custom_diffuser_hugging_face_id}\n",
        "\n",
        "#Import LoRA\n",
        "if custom_LoRA_url != \"None\":\n",
        "  %cd {file_path}/autoimport/lora\n",
        "  !wget -nc {file_path}/autoimport/lora {custom_LoRA_url}\n",
        "if HuggingFace_LoRA_Repo_ID != \"None\":\n",
        "  %cd {file_path}/autoimport/lora\n",
        "  !git clone --depth 1 --verbose --progress https://huggingface.co/{HuggingFace_LoRA_Repo_ID}\n",
        "\n",
        "#Import ControlNet\n",
        "if custom_ControlNet_url != \"None\":\n",
        "  %cd {file_path}/autoimport/controlnet\n",
        "  !wget -nc {file_path}/autoimport/controlnet {custom_ControlNet_url}\n",
        "if HuggingFace_ControlNet_Repo_ID != \"None\":\n",
        "  %cd {file_path}/autoimport/controlnet\n",
        "  !git clone --depth 1 --verbose --progress https://huggingface.co/{HuggingFace_ControlNet_Repo_ID}\n",
        "\n",
        "%cd {file_path}\n",
        "\n",
        "#Clear Output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "NO3XyDPsTJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnO01U-W6Yjp"
      },
      "outputs": [],
      "source": [
        "#@title 5. Start InvokeAI. { display-mode: \"form\" }\n",
        "#@markdown ### Starting the App\n",
        "#@markdown This step takes about 30 seconds to generate your URL but 2-3 mins before it will work fully!. <br>\n",
        "#@markdown Diffuser models that are auto imported are NOT selected by default and must be chosen from the menu in app.\n",
        "\n",
        "#@markdown ### Notes about connection types.\n",
        "#@markdown #### Localtunnel = Once it gets going it is quite stable but often has \"502\" Errors, You must wait for THEM to fix it, please try another connection type.\n",
        "#@markdown #### Serveo = Almost, always connects however, it can drop your connection if a HTML error occurs, Simply wait for the images to finish generating, stop an re-start this step.\n",
        "#@markdown #### NGROK = Very stable but requires a token, see the \"configuration\" step for more details.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "%cd {file_path}\n",
        "import os\n",
        "\n",
        "if connection_type == \"Serveo\":\n",
        "  !ssh -o StrictHostKeyChecking=no -o ServerAliveInterval=60 -R 80:localhost:9090 serveo.net & . {file_path}/.venv/bin/activate; invokeai-web --outdir={outpath}\n",
        "\n",
        "if connection_type == \"Localtunnel\":\n",
        "  print(\"How to connect to localtunnel:\");\n",
        "  print(\"A localtunnel Interface connection is generated here, To use this, please do the following \")\n",
        "  print(\"1. Copy this IP address\")\n",
        "  !curl ipv4.icanhazip.com\n",
        "  print(\"2. Click the random 'https://XXX-YYY-ZZZ.loca.lt' link that is generated below.\")\n",
        "  print(\"3. Paste the IP into the provided box and submit. \")\n",
        "  print(\" \")\n",
        "  print(\"Note: An error of '502 Bad Gateway' typically is an error at Localtunnels end. A '504 Gateway Time-out' Error means invokeai has not started yet.\")\n",
        "  print(\" \")\n",
        "  !npm install -g localtunnel\n",
        "  !npx localtunnel --port 9090 & . {file_path}/.venv/bin/activate; invokeai-web --outdir={outpath}\n",
        "\n",
        "if connection_type == \"NGROK\":\n",
        "  if ngrok_token == \"None\":\n",
        "    print(\"You have Selected NGROK but did not supply an NGROK token.\")\n",
        "    print(\"Falling back to a 'Serveo' connection type.\")\n",
        "    print(\"Please either add an NGROK token to step 1, re-run step 1, then re-run this step, or just re-run this step to use 'Servio'.\")\n",
        "    connection_type = \"Serveo\"\n",
        "  if ngrok_token != \"None\":\n",
        "    !pip install pyngrok --quiet\n",
        "    from pyngrok import ngrok\n",
        "    ngrok.kill()\n",
        "    ngrok.set_auth_token(ngrok_token)\n",
        "    public_url = ngrok.connect(9090).public_url\n",
        "    print(f'InvokeAI Public URL: {public_url}')\n",
        "    ! . {file_path}/.venv/bin/activate; invokeai-web --outdir={outpath}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}