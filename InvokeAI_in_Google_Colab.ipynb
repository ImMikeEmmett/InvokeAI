{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy86JeXPHQmW"
      },
      "source": [
        "# InvokeAI in Google Colab\n",
        "Make sure to enable GPU, Edit > Notebook Settings > Hardware accelerator > GPU <br>\n",
        "This takes between 10-12 mins to start fully, + about 3 mins per custom model.<br>\n",
        "To start, Click \"Runtime\" > \"Run All\". Alternaivly you can click the \"play\" button on each step below one after the other, No need to wait for the previous steps to finish as they will join a queue.<br>\n",
        "\n",
        "##NOTE: As of 2023-07-04, InvokeAI version 3.0.0 is in Beta. Both versions are available here. Please note: Version 3 is unsatble untill full release, at that time this comment will be removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cEAEBY2sFdGR"
      },
      "outputs": [],
      "source": [
        "#@title 1. Configuration { display-mode: \"form\" }\n",
        "#@markdown #Instance Type\n",
        "#@markdown Persistent = Stores images in your Google drive, in a folder called \"/invokeaiOutput\".<br>\n",
        "#@markdown Temporary = Images are stored in the runtime and are removed when the runtime ends or crashes. Make sure to download your images! <br>\n",
        "Version = \"2.3.5.post2 (Stable)\" #@param ['2.3.5.post2 (Stable)','Version 3 (Testing)'] {type:\"string\"}\n",
        "Type = \"Persistent\" #@param ['Persistent','Temporary'] {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #Xformers\n",
        "#@markdown Xformers makes this \"memory efficient\" so larger images (above 1024x1024) can be made, However it can be less stable and may generate blank outputs. It is recomended that you set it to \"Enabled\" unless you experiance issues with your models.\n",
        "xformers = \"Enabled\" #@param ['Enabled' ,'Disabled'] {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #NSFW Checker\n",
        "#@markdown This checks outputs for \"Not Safe For Work\" content and obscures the image. This is not 100% reliable.\n",
        "nsfw_checker = \"Enabled\" #@param ['Enabled' ,'Disabled'] {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #Custom Models\n",
        "#@markdown Custom Diffuser Model\n",
        "custom_diffuser_hugging_face_id = \"kebab111/crosskemono\" #@param [\"None\",\"kebab111/crosskemono\", \"dreamlike-art/dreamlike-photoreal-2.0\", \"stablediffusionapi/disney-pixal-cartoon\"] {allow-input: true}\n",
        "#@markdown Custom Safetensor or Checkpoint models <br>\n",
        "custom_model_url = \"None\" #@param [\"None\",\"https://huggingface.co/ffggggfg/Crosskemono/resolve/main/crosskemonoFurryModel_crosskemono25.safetensors\"] {allow-input: true}\n",
        "custom_vae_url = \"None\" #@param [\"None\",\"https://huggingface.co/ffggggfg/Crosskemono/resolve/main/Crosskemono2.vae.pt\"] {allow-input: true}\n",
        "\n",
        "#@markdown If you have selecfted a custom diffuser model will be auto imported and selectable from the dropdown menu in app. <br> <br>\n",
        "#@markdown Custom .safetensor or .checkpoint models are stored in /content/invokeai/CustomModels and must be manually added via the GUI after it starts. Your Custom VAE will be stored in /content/invokeai/CustomVae <br>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #NGROK\n",
        "#@markdown ####Recomended, as it is a more stable connection, but it not required.<br>\n",
        "#@markdown ####Sign up and get your free token at https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "#@markdown An NGROK token is needed for the more stable connection, however this project gives 2 options on how to connect, if you are happy with the less stable \"Localtunnel\" Leave this as \"None\".<br>\n",
        "ngrok_token = \"None\" #@param ['None'] {allow-input: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrEl_CcOHJYh"
      },
      "outputs": [],
      "source": [
        "#@title 2.1. Build and Configure. (V2.3.5.post2) { display-mode: \"form\" }\n",
        "#@markdown This takes about 10-12 mins to finish + some extra time to download custom Checkpoint / Safetensor models. { display-mode: \"form\" }\n",
        "%%capture\n",
        "if Version == \"2.3.5.post2 (Stable)\":\n",
        "  file_path = '/content/invokeai'\n",
        "\n",
        "  #Set up temporary storage if running in \"Temporary\" mode.\n",
        "  if Type == \"Temporary\":\n",
        "      outpath = '/content/invokeai/output'\n",
        "      import os\n",
        "      if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "  # Mount and set up Google drive if running in \"Persistent\" mode.\n",
        "  if Type == \"Persistent\":\n",
        "    outpath = '/content/drive/MyDrive/invokeaiOutput'\n",
        "    import os\n",
        "  from google.colab import drive\n",
        "  if not os.path.exists(file_path):\n",
        "    drive.mount('/content/drive')\n",
        "    if not os.path.exists(file_path):\n",
        "      os.makedirs(file_path)\n",
        "\n",
        "\n",
        "  #Install Python + Pip\n",
        "  !sudo apt-get update -y\n",
        "  !sudo apt-get install python3.9 python3-pip python3.9-distutils python3.9-dev libpython3.9-dev python3.9-venv -y\n",
        "\n",
        "  #Change alternatives\n",
        "  !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 3\n",
        "\n",
        "  #Update pip\n",
        "  %cd {file_path}\n",
        "  !curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "  !python3 get-pip.py --force-reinstall\n",
        "  !python -m pip install --upgrade pip\n",
        "  !pip install --upgrade setuptools\n",
        "  !pip install pillow==9.3.0\n",
        "\n",
        "  #Handle colab dependencies\n",
        "  !ln -s /usr/local/lib/python3.8/dist-packages/google /usr/local/lib/python3.9/dist-packages/google\n",
        "  !python3 -m pip install ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "  #Create InvokeAI root\n",
        "  import os\n",
        "  os.environ['INVOKEAI_ROOT'] = file_path\n",
        "  if not os.path.exists(file_path):\n",
        "    os.makedirs(invokeai_root)\n",
        "\n",
        "  #Create the virtual environment\n",
        "  %cd {file_path}\n",
        "  !python -m venv .venv --prompt InvokeAI\n",
        "  !source .venv/bin/activate; python -m pip install --upgrade pip\n",
        "  !source .venv/bin/activate; pip install InvokeAI[xformers]==2.3.5.post2 --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "  !source .venv/bin/activate; python -m pip uninstall torchmetrics==1.0.0 -y\n",
        "  !source .venv/bin/activate; python -m pip install torchmetrics==0.7.0\n",
        "  !mkdir {outpath}\n",
        "\n",
        "  #Edit invokeai.init and download default models\n",
        "  !cd {file_path}\n",
        "  !source .venv/bin/activate; invokeai-configure --yes\n",
        "  !sed -i 's/--max_loaded_models=2/--max_loaded_models=1/' invokeai.init\n",
        "  if xformers == \"Disabled\":\n",
        "    !sed -i 's/--xformers/--no-xformers/' invokeai.init\n",
        "  if nsfw_checker == \"Disabled\":\n",
        "    !sed -i 's/--nsfw_checker/--no-nsfw_checker/' invokeai.init\n",
        "\n",
        "\n",
        "  #Download custom checkpoint models\n",
        "  if custom_model_url != \"None\":\n",
        "    !mkdir {file_path}/CustomModels\n",
        "    %cd {file_path}/CustomModels\n",
        "    !wget -nc {file_path}/CustomModels {custom_model_url}\n",
        "\n",
        "  #Download custom VAE\n",
        "  if custom_vae_url != \"None\":\n",
        "    !mkdir {file_path}/CustomVae\n",
        "    %cd {file_path}/CustomVae\n",
        "    !wget -nc {file_path}/CustomVae {custom_vae_url}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.2. Build and Configure. (V3.0.0.4b) { display-mode: \"form\" }\n",
        "#@markdown This takes about 10-15 mins to finish + some extra time to download custom models. <br>\n",
        "#@markdown Verion 3 auto Imports all model typed Models at runtime.\n",
        "%%capture\n",
        "if Version == \"Version 3 (Testing)\":\n",
        "  file_path = '/content/invokeai'\n",
        "\n",
        "  #Set up temporary storage if running in \"Temporary\" mode.\n",
        "  if Type == \"Temporary\":\n",
        "      outpath = '/content/invokeai/output'\n",
        "      import os\n",
        "      if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "  # Mount and set up Google drive if running in \"Persistent\" mode.\n",
        "  if Type == \"Persistent\":\n",
        "    outpath = '/content/drive/MyDrive/invokeaiOutputV3'\n",
        "    import os\n",
        "    from google.colab import drive\n",
        "    if not os.path.exists(file_path):\n",
        "      drive.mount('/content/drive')\n",
        "      if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "\n",
        "  #Install Python + Pip\n",
        "  !sudo apt-get update -y\n",
        "  !sudo apt-get install python3.9 python3-pip python3.9-distutils python3.9-dev libpython3.9-dev python3.9-venv -y\n",
        "\n",
        "  #Change alternatives\n",
        "  !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 3\n",
        "\n",
        "  #Update pip\n",
        "  %cd {file_path}\n",
        "  !curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "  !python3 get-pip.py --force-reinstall\n",
        "  !python -m pip install --upgrade pip\n",
        "  !pip install --upgrade setuptools\n",
        "  !pip install pillow==9.3.0\n",
        "\n",
        "  #Handle colab dependencies\n",
        "  !ln -s /usr/local/lib/python3.8/dist-packages/google /usr/local/lib/python3.9/dist-packages/google\n",
        "  !python3 -m pip install ipython ipython_genutils ipykernel jupyter_console prompt_toolkit httplib2 astor\n",
        "\n",
        "  #Create InvokeAI root\n",
        "  import os\n",
        "  os.environ['INVOKEAI_ROOT'] = file_path\n",
        "  if not os.path.exists(file_path):\n",
        "    os.makedirs(invokeai_root)\n",
        "\n",
        "  #Create the virtual environment\n",
        "  %cd {file_path}\n",
        "  !python -m venv .venv --prompt InvokeAI\n",
        "  !source .venv/bin/activate; python -m pip install --upgrade pip\n",
        "  !source .venv/bin/activate; pip install \"invokeai @ https://github.com/invoke-ai/InvokeAI/archive/refs/tags/v3.0.0+b4.zip\" --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "  #Once PIP installs are available, Migrate to the below install option, instead of the above GIT hub pull.\n",
        "  #!source .venv/bin/activate; pip install InvokeAI[xformers]==VERSION --use-pep517 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "  !mkdir {outpath}\n",
        "\n",
        "  #Edit invokeai.init and download default models\n",
        "  !cd {file_path}\n",
        "  !source .venv/bin/activate; invokeai-configure --yes\n",
        "  !sed -i 's/max_loaded_models: 2/max_loaded_models: 1/' invokeai.yaml\n",
        "  if xformers == \"Disabled\":\n",
        "    !sed -i 's/xformers_enabled: true/xformers_enabled: false/' invokeai.yaml\n",
        "  if nsfw_checker == \"Disabled\":\n",
        "    !sed -i 's/nsfw_checker: true/nsfw_checker: false/' invokeai.yaml\n",
        "\n",
        "\n",
        "  #Download custom checkpoint models\n",
        "  if custom_model_url != \"None\":\n",
        "    #!mkdir {file_path}/CustomModels\n",
        "    #%cd {file_path}/CustomModels\n",
        "    #!wget -nc {file_path}/CustomModels {custom_model_url}\n",
        "    !mkdir {file_path}/CustomModels\n",
        "    %cd {file_path}/CustomModels\n",
        "    !wget -nc {file_path}/autoimport/main {custom_model_url}\n",
        "\n",
        "\n",
        "  #Download custom VAE\n",
        "  if custom_vae_url != \"None\":\n",
        "    #!mkdir {file_path}/CustomVae\n",
        "    #%cd {file_path}/CustomVae\n",
        "    #!wget -nc {file_path}/CustomVae {custom_vae_url}\n",
        "    %cd {file_path}/autoimport/vae\n",
        "    !wget -nc {file_path}/CustomVae {custom_vae_url}\n",
        "\n",
        "  #Import Diffuser Model\n",
        "  if custom_diffuser_hugging_face_id != \"None\":\n",
        "    %cd {file_path}/autoimport/main\n",
        "    !git clone https://huggingface.co/{custom_diffuser_hugging_face_id}\n",
        "\n",
        "  %cd {file_path}"
      ],
      "metadata": {
        "id": "NO3XyDPsTJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiWohNwq6UHN"
      },
      "outputs": [],
      "source": [
        "#@title 3. Connect via NGROK - Recomended. { display-mode: \"form\" }\n",
        "#@markdown This link will start working when the next step tells you to connect to localhost.<br>\n",
        "#@markdown If you are not using NGROK or if NGROK fails to start, See the next step for an alternate connection method.<br>\n",
        "#@markdown You can ignore this step if you have not suplied a NGROK token in the configuration section.\n",
        "\n",
        "if ngrok_token != \"None\":\n",
        "  %cd {file_path}\n",
        "  !pip install pyngrok --quiet\n",
        "  from pyngrok import ngrok\n",
        "  ngrok.kill()\n",
        "  ngrok.set_auth_token(ngrok_token)\n",
        "  public_url = ngrok.connect(9090).public_url\n",
        "  print(f'InvokeAI Public URL: {public_url}')\n",
        "\n",
        "if ngrok_token == \"None\":\n",
        "  print(\"You have not suppied an NGROK token.\")\n",
        "  print(\"Please follow the connection process for localtunnel below.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnO01U-W6Yjp"
      },
      "outputs": [],
      "source": [
        "#@title 4. Start InvokeAI + \"localtunnel\" { display-mode: \"form\" }\n",
        "#@markdown ### Starting the App\n",
        "#@markdown This step takes either 2-3 mins to start InvokeAI or 5-6 mins if you have selected a diffuser model. <br>\n",
        "#@markdown Diffuser models that are auto imported are NOT selected by default and must be chosen from the dropdown menu in the top right.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### How to connect to localtnnel\n",
        "#@markdown A \"localtunnel\" Interface connection is generated here, To use this, please do the following: <br>\n",
        "#@markdown 1) Copy the IP address appears in line 2 or 3 of this cells output. <br>\n",
        "#@markdown 2) Click the random \"https://XXX-YYY-ZZZ.loca.lt\" link that is generated in (Aproximatly) like 8. <br>\n",
        "#@markdown 3) Paste the IP into the provided box and submit. <br>\n",
        "#@markdown Note: An error of \"502 Bad Gateway\" typically is an error at Localtunnels end.<br><br>\n",
        "if Version == \"2.3.5.post2 (Stable)\":\n",
        "  %cd {file_path}\n",
        "  print(\"Localtunnel IP address:\")\n",
        "  !curl ipv4.icanhazip.com\n",
        "  !npm install -g localtunnel\n",
        "  if custom_diffuser_hugging_face_id != \"None\":\n",
        "    !npx localtunnel --port 9090 & !source .venv/bin/activate; invokeai --web --host 0.0.0.0 --outdir={outpath} --autoimport {custom_diffuser_hugging_face_id}\n",
        "  if custom_diffuser_hugging_face_id == \"None\":\n",
        "    !npx localtunnel --port 9090 & !source .venv/bin/activate; invokeai --web --host 0.0.0.0 --outdir={outpath}\n",
        "\n",
        "\n",
        "if Version == \"Version 3 (Testing)\":\n",
        "  %cd {file_path}\n",
        "  print(\"Localtunnel IP address:\")\n",
        "  !curl ipv4.icanhazip.com\n",
        "  !npm install -g localtunnel\n",
        "  !npx localtunnel --port 9090 & !source .venv/bin/activate; invokeai-web --outdir={outpath}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5. Temporary Mode - Zip Output for Download. { display-mode: \"form\" }\n",
        "#@markdown Google Colab does not give a nice way to download multiple files at once.<br>\n",
        "#@markdown When running in Temporary Mode, This will compress all images into a single \"output.zip\" to make downloading everything simple!<br>\n",
        "#@markdown This file can be downloaded using the Colab file browser: /invokeai/output/Output.zip <br>\n",
        "#@markdown For this step to run you must manually stop step 4.\n",
        "\n",
        "if Type == \"Temporary\":\n",
        "  !mkdir /content/zip\n",
        "  !rm {outpath}/Output.zip\n",
        "  !rm -r /content/zip/*\n",
        "  !cp -p {outpath}/*png /content/zip\n",
        "  !cd /content/zip/\n",
        "  !zip {outpath}/Output.zip /content/zip/*.png"
      ],
      "metadata": {
        "id": "V6GVH_Km8JgK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}